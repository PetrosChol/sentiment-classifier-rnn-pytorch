{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **\"Step-by-Step Guide to Sentiment Classification with PyTorch RNN\"**"
      ],
      "metadata": {
        "id": "v8DthCTKqlVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Libraries"
      ],
      "metadata": {
        "id": "EUg2mYGMquqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pandas textblob nltk scikit-learn torch torchvision torchaudio torchtext datasets tokenizers torchmetrics tensorboard altair wandb spacy torchinfo"
      ],
      "metadata": {
        "id": "4kyJkrtbqZWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all necessary libraries for our task\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Download nltk resources too for some text processing\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "Pfb1sLWLqZU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load and Inspect Data\n",
        "\n",
        "Load our dataset and select the 'text' and 'sentiment' columns."
      ],
      "metadata": {
        "id": "svncN9B2rL5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our data is in a CSV file named 'train.csv'\n",
        "# We are using 'ISO-8859-1' encoding, because 'utf-8' can't decode properly our data\n",
        "df = pd.read_csv('train.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Select only 'text' and 'sentiment' columns for our purpose\n",
        "df = df[['text', 'sentiment']]\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9AVYiX90qZSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It's always a good practice to check our\n",
        "# target variable's (label) distribution for imbalances\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "# As we can see below there are not any major imbalaces in our dataset"
      ],
      "metadata": {
        "id": "eAPiNHyMqZQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "This is a crucial step. We'll perform the following preprocessing steps:\n",
        "\n",
        "   - Lowercase: Convert all text to lowercase.\n",
        "\n",
        "   - Remove Punctuation: Remove punctuation marks.\n",
        "\n",
        "   - Remove Stop Words: Remove common words that don't carry much sentiment information (like \"the\", \"a\", \"is\").\n",
        "\n",
        "   - Remove URLs: Generally a very good practice when preprocessing text data for sentiment analysis, especially for tweets (Noise Reduction, Focus on Textual Content, Vocabulary Size Reduction, Improved Generalization)"
      ],
      "metadata": {
        "id": "63gozVPjtgMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_st(text):\n",
        "    if isinstance(text, str):\n",
        "\n",
        "        # Lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs using regular expression\n",
        "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "\n",
        "        # Remove punctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        # Remove stop words\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        text_list = text.split()\n",
        "        filtered_words = [word for word in text_list if word not in stop_words]\n",
        "        text = ' '.join(filtered_words)\n",
        "        return text\n",
        "    else:\n",
        "        return \" \"\n",
        "\n",
        "df['processed_text_st'] = df['text'].apply(preprocess_text_st)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "XDApdXcMqZIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Sentiment Encoding\n",
        "\n",
        "We need to convert the sentiment labels (e.g., 'negative', 'neutral', 'positive') into numerical values that our model can understand. Let's create a mapping:\n",
        "\n",
        "   - 'negative': 0\n",
        "\n",
        "   - 'neutral': 1\n",
        "\n",
        "   - 'positive': 2"
      ],
      "metadata": {
        "id": "fG53jHthv6Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "df['sentiment_encoded'] = df['sentiment'].map(sentiment_map)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Md9qZ-j3qZDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Generate Sentence Embeddings using Sentence Transformer\n",
        "\n",
        "We will use `SentenceTransformer` to encode our processed tweets into sentence embeddings."
      ],
      "metadata": {
        "id": "Ioau4hxIwY6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Sentence Transformer model\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "sentence_model = SentenceTransformer(model_name)\n",
        "\n",
        "# Generate sentence embeddings for all processed texts\n",
        "# convert_to_tensor=True makes it return PyTorch tensors directly, which is convenient\n",
        "sentence_embeddings = sentence_model.encode(df['processed_text_st'].tolist(), convert_to_tensor=True)\n",
        "\n",
        "# Move the tensor to CPU before converting to list for DataFrame\n",
        "sentence_embeddings_cpu = sentence_embeddings.cpu()\n",
        "\n",
        "# Add embeddings to the DataFrame (that's optional, but can be useful for inspection)\n",
        "df['sentence_embedding'] = list(sentence_embeddings_cpu)\n",
        "\n",
        "print(f\"Shape of sentence embeddings: {sentence_embeddings.shape}\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "x4spVSXqqZBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Split Data into Training and Testing Sets (using Embeddings)\n",
        "\n",
        "We split the data based on the generated sentence embeddings and encoded sentiments."
      ],
      "metadata": {
        "id": "vACVpeZa1meI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since df['sentence_embedding'] is a list of PyTorch tensors,\n",
        "# we use torch.stack() to combine them into a single tensor X_st.\n",
        "# This tensor X_st will be our input features for the model.\n",
        "X_st = torch.stack(list(df['sentence_embedding']))\n",
        "y_st = df['sentiment_encoded']\n",
        "\n",
        "X_train_st, X_test_st, y_train_st, y_test_st = train_test_split(X_st, y_st, test_size=0.2, random_state=42, stratify=y_st)\n",
        "\n",
        "print(f\"Training embeddings shape: {X_train_st.shape}\")\n",
        "print(f\"Testing embeddings shape: {X_test_st.shape}\")\n",
        "print(f\"Training samples: {len(X_train_st)}\")\n",
        "print(f\"Testing samples: {len(X_test_st)}\")"
      ],
      "metadata": {
        "id": "COEkedrmwnMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Create PyTorch Dataset and DataLoader (for Sentence Embeddings)\n",
        "\n",
        "The Dataset and DataLoader are simplified because we are now dealing with fixed-size sentence embeddings, not sequences. No padding needed!"
      ],
      "metadata": {
        "id": "AHS-STGZ2LUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # embedding is already a tensor\n",
        "        embedding = self.embeddings[idx]\n",
        "        label = self.labels.iloc[idx]\n",
        "        return embedding, torch.tensor(label)"
      ],
      "metadata": {
        "id": "ntj77P15wnJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = SentimentDataset(X_train_st, y_train_st)\n",
        "test_dataset = SentimentDataset(X_test_st, y_test_st)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 64\n",
        "train_dataloader_st = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader_st = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Example of a batch\n",
        "for embedding_batch, label_batch in train_dataloader_st:\n",
        "    print(\"Embedding batch shape:\", embedding_batch.shape)\n",
        "    print(\"Label batch shape:\", label_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "T2-4AomHwnHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Define a Simple Feedforward Neural Network Model (MLP)\n",
        "\n",
        "   - This is a simple MLP with two fully connected layers, a ReLU activation, and dropout for regularization.\n",
        "\n",
        "   - embedding_dim: The input dimension is the dimension of the sentence embeddings (e.g., 384 for all-MiniLM-L6-v2).\n",
        "\n",
        "   - We don't need an embedding layer here because the input is already sentence embeddings.\n",
        "\n",
        "   - The output layer self.fc2 directly outputs logits (raw scores), and we rely on nn.CrossEntropyLoss to handle the Softmax internally."
      ],
      "metadata": {
        "id": "Vt-t62wI23EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, output_dim, hidden_dim, dropout_prob):\n",
        "        super().__init__()\n",
        "\n",
        "        # First fully connected layer\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "        # Second fully connected layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, embedding):\n",
        "\n",
        "        # Input is sentence embedding directly\n",
        "        # embedding shape: [batch_size, embedding_dim]\n",
        "        out = self.fc1(embedding)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # No softmax here, CrossEntropyLoss will handle it\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = sentence_embeddings.shape[1]\n",
        "hidden_dim = 256\n",
        "output_dim = len(sentiment_map)\n",
        "dropout_prob = 0.5\n",
        "\n",
        "model = SentimentClassifier(embedding_dim, output_dim, hidden_dim, dropout_prob)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "RgUnQU9iwnFS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Training the Sentiment Classifier Model\n",
        "\n",
        "This step focuses on training our feedforward neural network (`SentimentClassifier`) to classify sentiment based on the sentence embeddings generated by the `Sentence Transformer`. We will define the loss function, optimizer, and implement the training and evaluation loops.\n",
        "\n",
        "**9.1 Loss Function:**\n",
        "\n",
        "We will use **`nn.CrossEntropyLoss()`** as our loss function. This is a standard loss function for multi-class classification problems. It's well-suited for our task because:\n",
        "\n",
        "*   Sentiment analysis here is a multi-class problem (negative, neutral, positive). `CrossEntropyLoss` is designed for this scenario.\n",
        "*   `CrossEntropyLoss` expects the model's output to be raw, unnormalized scores which it then internally converts into probability distributions using a Softmax function. It compares this predicted probability distribution with the true class label (which we have encoded as integers: 0, 1, 2).\n",
        "*   The loss function quantifies how well our model's predictions match the true sentiment labels. The training process will aim to minimize this loss.\n",
        "\n",
        "**9.2 Optimizer:**\n",
        "\n",
        "We will use the **`optim.Adam()`** optimizer. **Adam** is a popular and effective optimization algorithm for training neural networks. **Adam** adapts the learning rate for each parameter individually during training, which often leads to faster convergence and better performance compared to optimizers with a fixed learning rate (like Stochastic Gradient Descent, SGD, with a constant learning rate).\n",
        "\n",
        "**9.3 Training Function (`train`):**\n",
        "\n",
        "The `train` function performs one epoch of training on the training dataset. Let's break down its steps:\n",
        "\n",
        "1.  **`model.train()`:**  Sets the model to training mode. This is important because some layers (like Dropout and BatchNorm, if used) behave differently during training and evaluation.\n",
        "2.  **`epoch_loss = 0`, `epoch_acc = 0`:** Initializes variables to accumulate the loss and accuracy for the entire epoch.\n",
        "3.  **`for batch_idx, (embedding_batch, label_batch) in enumerate(dataloader):`:** Iterates through the training data loader, which provides batches of sentence embeddings and corresponding sentiment labels.\n",
        "4.  **`embedding_batch = embedding_batch.to(device)`, `label_batch = label_batch.to(device)`:** Moves the input batch of embeddings and labels to the specified device (GPU if available, otherwise CPU). This ensures that computations are performed on the correct device.\n",
        "5.  **`optimizer.zero_grad()`:** Clears the gradients from the previous batch. Gradients accumulate by default in PyTorch, so we need to reset them before each new batch.\n",
        "6.  **`predictions = model(embedding_batch)`:** Performs the forward pass. The `embedding_batch` is fed into the `model`, and it outputs the predicted logits for each sentiment class.\n",
        "7.  **`loss = criterion(predictions, label_batch)`:** Calculates the loss by comparing the model's `predictions` with the `label_batch` using the `criterion` (CrossEntropyLoss).\n",
        "8.  **`acc = calculate_accuracy(predictions, label_batch)`:** Calculates the accuracy of the predictions for the current batch using the `calculate_accuracy` function we defined earlier.\n",
        "9.  **`loss.backward()`:** Performs the backward pass (backpropagation). This calculates the gradients of the loss with respect to the model's parameters.\n",
        "10. **`optimizer.step()`:** Updates the model's parameters using the calculated gradients and the optimization algorithm (Adam). This is where the model learns from the data.\n",
        "11. **`epoch_loss += loss.item()`, `epoch_acc += acc.item()`:** Accumulates the loss and accuracy for the current batch to calculate the average loss and accuracy for the entire epoch later.\n",
        "12. **`return epoch_loss / len(dataloader), epoch_acc / len(dataloader)`:** After processing all batches in the epoch, it returns the average training loss and average training accuracy for the epoch.\n",
        "\n",
        "**9.4 Evaluation Function (`evaluate`):**\n",
        "\n",
        "The `evaluate` function evaluates the model's performance on a given dataset (typically the test dataset) *without* updating the model's parameters.  It's crucial to evaluate on a separate dataset to assess how well the model generalizes to unseen data.\n",
        "\n",
        "1.  **`model.eval()`:** Sets the model to evaluation mode. This ensures that layers like Dropout and BatchNorm behave in evaluation mode (e.g., Dropout is turned off).\n",
        "2.  **`epoch_loss = 0`, `epoch_acc = 0`:** Initializes variables to accumulate loss and accuracy for the evaluation dataset.\n",
        "3.  **`with torch.no_grad():`:**  This context manager is important during evaluation. It disables gradient calculations. This is because we don't need to compute gradients during evaluation, which saves memory and computation time.\n",
        "4.  **The rest of the steps within the `with torch.no_grad():` block are very similar to the training function**, except:\n",
        "    *   **`optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()` are *not* present.**  We are only performing a forward pass to get predictions and calculate metrics, not updating the model's weights.\n",
        "5.  **`return epoch_loss / len(dataloader), epoch_acc / len(dataloader)`:** Returns the average evaluation loss and accuracy.\n",
        "\n",
        "**9.5 Training Loop:**\n",
        "\n",
        "The main training loop orchestrates the training process over multiple epochs:\n",
        "\n",
        "1.  **`for epoch in range(num_epochs):`:** Iterates for a predefined number of epochs. An epoch is one complete pass through the entire training dataset.\n",
        "2.  **`train_loss, train_acc = train(...)`:** Calls the `train` function to train the model for one epoch on the training data and get the training loss and accuracy.\n",
        "3.  **`test_loss, test_acc = evaluate(...)`:** Calls the `evaluate` function to evaluate the model on the test data and get the test loss and accuracy.\n",
        "4.  **`print(f'Epoch: ...')`:** Prints the epoch number, training loss, training accuracy, test loss, and test accuracy for each epoch. This allows us to monitor the training progress and observe how the model is learning and generalizing.\n",
        "\n",
        "By running this training loop, we iteratively refine our `SentimentClassifier` model to become better at classifying the sentiment of tweets based on their sentence embeddings. We will monitor the training and test set performance to decide when to stop training and to evaluate the final model's effectiveness."
      ],
      "metadata": {
        "id": "Wk8Ef9h04LWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper parameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "num_epochs = 100\n",
        "\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    \"Calculates the accuracy of predictions.\"\n",
        "\n",
        "    # Get the class with highest probability\n",
        "    _, predicted_classes = torch.max(predictions, 1)\n",
        "    correct_predictions = (predicted_classes == labels).sum().item()\n",
        "    accuracy = correct_predictions / len(labels)\n",
        "    return torch.tensor(accuracy)\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    for batch_idx, (embedding_batch, label_batch) in enumerate(dataloader):\n",
        "        embedding_batch = embedding_batch.to(device)\n",
        "        label_batch = label_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(embedding_batch) # Pass embeddings\n",
        "        loss = criterion(predictions, label_batch)\n",
        "        acc = calculate_accuracy(predictions, label_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (embedding_batch, label_batch) in enumerate(dataloader):\n",
        "            embedding_batch = embedding_batch.to(device)\n",
        "            label_batch = label_batch.to(device)\n",
        "\n",
        "            predictions = model(embedding_batch)\n",
        "            loss = criterion(predictions, label_batch)\n",
        "            acc = calculate_accuracy(predictions, label_batch)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_dataloader_st, optimizer, criterion, device)\n",
        "    test_loss, test_acc = evaluate(model, test_dataloader_st, criterion, device)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "vLBOkjpvwnC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Evaluate on Test Set"
      ],
      "metadata": {
        "id": "QBfjWJtv9Qwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_dataloader_st, criterion_st, device)\n",
        "print(f'Final Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "VpaG3MP6qY-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Make Predictions on New Text"
      ],
      "metadata": {
        "id": "EZEUBZ1o9i3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(model, sentence_transformer_model, sentence, device, sentiment_map_inv):\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess using ST preprocessing\n",
        "    processed_sentence = preprocess_text_st(sentence)\n",
        "\n",
        "    # Encode sentence\n",
        "    embedding = sentence_transformer_model.encode([processed_sentence], convert_to_tensor=True).to(device)\n",
        "\n",
        "    prediction = model(embedding)\n",
        "    _, predicted_class = torch.max(prediction, 1)\n",
        "    predicted_sentiment_index = predicted_class.item()\n",
        "    predicted_sentiment = sentiment_map_inv[predicted_sentiment_index]\n",
        "    return predicted_sentiment\n",
        "\n",
        "\n",
        "sentiment_map_inv = {v: k for k, v in sentiment_map.items()}\n",
        "\n",
        "# Test for a positive tweet\n",
        "positive_tweet = \"This movie was absolutely fantastic! I loved it.\"\n",
        "\n",
        "predicted_sentiment = predict_sentiment(model, sentence_model, positive_tweet, device, sentiment_map_inv)\n",
        "print(f\"Predicted sentiment for: '{positive_tweet}' is: {predicted_sentiment}\")\n",
        "\n",
        "# Test for a negative tweet\n",
        "tweet_negative = \"I'm feeling really down and upset today.\"\n",
        "\n",
        "predicted_sentiment_neg = predict_sentiment(model, sentence_model, tweet_negative, device, sentiment_map_inv)\n",
        "print(f\"Predicted sentiment for: '{tweet_negative}' is: {predicted_sentiment_neg}\")\n",
        "\n",
        "# Test for a neutral tweet\n",
        "tweet_neutral = \"I'm feeling normal today.\"\n",
        "\n",
        "predicted_sentiment_neu = predict_sentiment(model, sentence_model, tweet_neutral, device, sentiment_map_inv)\n",
        "print(f\"Predicted sentiment for: '{tweet_neutral}' is: {predicted_sentiment_neu}\")"
      ],
      "metadata": {
        "id": "bkGNfbuZqY8F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}